# üöÄ Guia Completo de Troca de Modelos Ollama no GPTPOL

## üìã Li√ß√µes Aprendidas da √öltima Troca

Durante a √∫ltima troca de modelos, identificamos problemas cr√≠ticos que precisam ser evitados:

1. **Incompatibilidade .env vs Backend:** O backend continuou usando um modelo antigo mesmo ap√≥s a atualiza√ß√£o do arquivo .env
2. **Erro 500:** O modelo especificado n√£o estava dispon√≠vel no Ollama
3. **Reinicializa√ß√£o parcial:** Nem todos os servi√ßos foram corretamente reiniciados ap√≥s as altera√ß√µes

## üìÅ Arquivos que Precisam ser Modificados

| Arquivo | Localiza√ß√£o | Tipo de Modifica√ß√£o | Import√¢ncia |
|---------|------------|---------------------|-------------|
| `.env` | `/mnt/projetos_nexor/Servidor_Nexortech/Projetos/GPTPOL_OLLAMA/.env` | Vari√°vel `OLLAMA_MODEL` | **CR√çTICA** |
| `main_api.py` | `/mnt/projetos_nexor/Servidor_Nexortech/Projetos/GPTPOL_OLLAMA/backend/main_api.py` | Par√¢metros do modelo | **CR√çTICA** |
| `typewriter.js` | `/mnt/projetos_nexor/Servidor_Nexortech/Projetos/GPTPOL_OLLAMA/frontend/config/typewriter.js` | Configura√ß√µes de velocidade | Recomendada |

## üîç Detalhamento das Modifica√ß√µes por Arquivo

### 1. Arquivo `.env`

**Localiza√ß√£o:** `/mnt/projetos_nexor/Servidor_Nexortech/Projetos/GPTPOL_OLLAMA/.env`

**Descri√ß√£o:** Arquivo de configura√ß√£o de ambiente que define o modelo a ser utilizado pelo backend.

**Modifica√ß√£o necess√°ria:**
- Alterar a vari√°vel `OLLAMA_MODEL` para o nome do novo modelo

**Exemplo:**
```
# Configura√ß√µes Gerais do GPTPOL
API_URL=http://192.168.0.241:8000
FRONTEND_URL=http://192.168.0.241:3000
OLLAMA_API_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=gemma2:27b  # <-- Modificar esta linha
```

**Comando para modificar:**
```bash
echo -e "# Configura√ß√µes Gerais do GPTPOL\nAPI_URL=http://192.168.0.241:8000\nFRONTEND_URL=http://192.168.0.241:3000\nOLLAMA_API_URL=http://localhost:11434/api/generate\nOLLAMA_MODEL=NOVO_MODELO" > .env
```

### 2. Arquivo `main_api.py`

**Localiza√ß√£o:** `/mnt/projetos_nexor/Servidor_Nexortech/Projetos/GPTPOL_OLLAMA/backend/main_api.py`

**Descri√ß√£o:** API principal do backend que cont√©m a comunica√ß√£o com o modelo Ollama e os par√¢metros de configura√ß√£o do modelo.

**Se√ß√µes que precisam ser modificadas:**

1. **Fun√ß√£o `gerar_resposta_ollama` (aproximadamente linha 130-170):**
   - Modificar a se√ß√£o `options` para os par√¢metros recomendados do novo modelo

   ```python
   def gerar_resposta_ollama(prompt):
       try:
           payload = {
               "model": OLLAMA_MODEL,
               "prompt": prompt,
               "stream": False,
               "options": {
                   "num_ctx": 4096,           # Ajustar para o modelo espec√≠fico
                   "num_thread": 12,          # Ajustar para o modelo espec√≠fico
                   "temperature": 0.7,        # Ajustar para o modelo espec√≠fico
                   "top_k": 40,               # Ajustar para o modelo espec√≠fico
                   "top_p": 0.9,              # Ajustar para o modelo espec√≠fico
                   "num_predict": 512,        # Ajustar para o modelo espec√≠fico
                   "repeat_penalty": 1.1,     # Ajustar para o modelo espec√≠fico
                   "stop": ["<end>", "</answer>"] # Ajustar para o modelo espec√≠fico
               }
           }
   ```

2. **Fun√ß√£o `gerar_resposta_ollama_stream` (aproximadamente linha 230-270):**
   - Modificar a se√ß√£o `options` para os par√¢metros recomendados do novo modelo
   - **IMPORTANTE:** Os par√¢metros desta se√ß√£o devem ser id√™nticos aos da fun√ß√£o acima

   ```python
   async def gerar_resposta_ollama_stream(prompt):
       try:
           payload = {
               "model": OLLAMA_MODEL,
               "prompt": prompt,
               "stream": True,  # Habilitando streaming
               "options": {
                   "num_ctx": 4096,           # Ajustar para o modelo espec√≠fico
                   "num_thread": 12,          # Ajustar para o modelo espec√≠fico
                   "temperature": 0.7,        # Ajustar para o modelo espec√≠fico
                   "top_k": 40,               # Ajustar para o modelo espec√≠fico
                   "top_p": 0.9,              # Ajustar para o modelo espec√≠fico
                   "num_predict": 512,        # Ajustar para o modelo espec√≠fico
                   "repeat_penalty": 1.1,     # Ajustar para o modelo espec√≠fico
                   "stop": ["<end>", "</answer>"] # Ajustar para o modelo espec√≠fico
               }
           }
   ```

3. **Fun√ß√µes `perguntar_stream` e `perguntar` (aproximadamente linhas 290-340):**
   - Verificar e ajustar o `prompt_context` se necess√°rio para o modelo espec√≠fico:

   ```python
   # Instru√ß√£o espec√≠fica para o assistente jur√≠dico
   prompt_context = "Voc√™ √© um assistente jur√≠dico policial brasileiro. Responda de forma t√©cnica, direta e concisa em portugu√™s correto: "
   ```

### 3. Arquivo `typewriter.js`

**Localiza√ß√£o:** `/mnt/projetos_nexor/Servidor_Nexortech/Projetos/GPTPOL_OLLAMA/frontend/config/typewriter.js`

**Descri√ß√£o:** Configura√ß√µes do efeito de digita√ß√£o que pode precisar de ajustes dependendo da velocidade de resposta do modelo.

**Modifica√ß√µes recomendadas para modelos mais r√°pidos (ex: Mistral):**
```javascript
const typewriterConfig = {
  // Velocidade base de digita√ß√£o em milissegundos (menor = mais r√°pido)
  baseSpeed: 10,  // Reduzir para modelos mais r√°pidos
  
  // Varia√ß√£o de velocidade - multiplicador m√≠nimo e m√°ximo
  speedVariation: {
    min: 0.7,  // Ajustar para digita√ß√£o mais r√°pida
    max: 1.2   // Ajustar para digita√ß√£o mais r√°pida
  },
  
  // Efeito de "pensar" - pausa antes de come√ßar a digitar (ms)
  thinkingDelay: 200,  // Reduzir para modelos mais r√°pidos
  
  // Outros par√¢metros...
};
```

**Modifica√ß√µes recomendadas para modelos mais lentos (ex: Llama 3 70B):**
```javascript
const typewriterConfig = {
  // Velocidade base de digita√ß√£o em milissegundos (menor = mais r√°pido)
  baseSpeed: 15,  // Aumentar para modelos mais lentos
  
  // Varia√ß√£o de velocidade - multiplicador m√≠nimo e m√°ximo
  speedVariation: {
    min: 0.8,  // Ajustar para digita√ß√£o mais consistente
    max: 1.4   // Ajustar para digita√ß√£o mais variada
  },
  
  // Efeito de "pensar" - pausa antes de come√ßar a digitar (ms)
  thinkingDelay: 400,  // Aumentar para simular o tempo de processamento
  
  // Outros par√¢metros...
};
```

## üîß Ajustes de Par√¢metros por Modelo

### Gemma 2 (27B)

**Arquivo `main_api.py` - Se√ß√£o options:**
```python
"options": {
    "num_ctx": 4096,           
    "num_thread": 12,          
    "temperature": 0.7,        
    "top_k": 40,               
    "top_p": 0.9,              
    "num_predict": 512,        
    "repeat_penalty": 1.1,     
    "stop": ["<end>", "</answer>"] 
}
```

**Arquivo `typewriter.js`:**
```javascript
baseSpeed: 12,
speedVariation: {
  min: 0.8,
  max: 1.3
},
thinkingDelay: 250,
```

### Llama 3 (70B)

**Arquivo `main_api.py` - Se√ß√£o options:**
```python
"options": {
    "num_ctx": 4096,           
    "num_thread": 12,          
    "temperature": 0.7,        
    "top_k": 50,               
    "top_p": 0.95,             
    "num_predict": 1024,       
    "repeat_penalty": 1.1,     
    "stop": ["<|end_of_text|>", "</s>"] 
}
```

**Arquivo `typewriter.js`:**
```javascript
baseSpeed: 15,
speedVariation: {
  min: 0.8,
  max: 1.4
},
thinkingDelay: 400,
```

### Mistral (7B)

**Arquivo `main_api.py` - Se√ß√£o options:**
```python
"options": {
    "num_ctx": 4096,           
    "num_thread": 8,           
    "temperature": 0.7,        
    "top_k": 40,               
    "top_p": 0.9,              
    "num_predict": 512,        
    "repeat_penalty": 1.1     
}
```

**Arquivo `typewriter.js`:**
```javascript
baseSpeed: 10,
speedVariation: {
  min: 0.7,
  max: 1.2
},
thinkingDelay: 200,
```

### Phi-3 (14B)

**Arquivo `main_api.py` - Se√ß√£o options:**
```python
"options": {
    "num_ctx": 4096,           
    "num_thread": 8,           
    "temperature": 0.8,        
    "top_k": 40,               
    "top_p": 0.9,              
    "num_predict": 512,        
    "repeat_penalty": 1.1     
}
```

**Arquivo `typewriter.js`:**
```javascript
baseSpeed: 12,
speedVariation: {
  min: 0.7,
  max: 1.3
},
thinkingDelay: 250,
```

### Mixtral (8x7B)

**Arquivo `main_api.py` - Se√ß√£o options:**
```python
"options": {
    "num_ctx": 4096,           
    "num_thread": 10,          
    "temperature": 0.7,        
    "top_k": 40,               
    "top_p": 0.9,              
    "num_predict": 512,        
    "repeat_penalty": 1.1     
}
```

**Arquivo `typewriter.js`:**
```javascript
baseSpeed: 12,
speedVariation: {
  min: 0.8,
  max: 1.3
},
thinkingDelay: 300,
```

## üìù Processo Completo de Troca de Modelo

1. **Parar todos os servi√ßos:**
   ```bash
   bash stop-gptpol.sh
   ```

2. **Verificar modelo atual:**
   ```bash
   cat .env | grep OLLAMA_MODEL
   ```

3. **Listar e remover o modelo atual:**
   ```bash
   ollama list
   ollama rm MODELO_ATUAL
   ```

4. **Instalar o novo modelo:**
   ```bash
   ollama pull NOVO_MODELO
   ```

5. **Atualizar arquivo .env:**
   ```bash
   echo -e "# Configura√ß√µes Gerais do GPTPOL\nAPI_URL=http://192.168.0.241:8000\nFRONTEND_URL=http://192.168.0.241:3000\nOLLAMA_API_URL=http://localhost:11434/api/generate\nOLLAMA_MODEL=NOVO_MODELO" > .env
   ```

6. **Editar o arquivo main_api.py:**
   ```bash
   nano backend/main_api.py
   ```
   - Localizar e modificar as duas se√ß√µes `options` nas fun√ß√µes `gerar_resposta_ollama` e `gerar_resposta_ollama_stream`
   - Salvar com Ctrl+O, Enter, Ctrl+X

7. **[Opcional] Editar arquivo typewriter.js:**
   ```bash
   nano frontend/config/typewriter.js
   ```
   - Ajustar par√¢metros para o novo modelo conforme recomenda√ß√µes acima
   - Salvar com Ctrl+O, Enter, Ctrl+X

8. **Iniciar servi√ßos:**
   ```bash
   bash start-gptpol.sh
   ```

9. **Verificar logs se necess√°rio:**
   ```bash
   cat logs/backend.log
   cat logs/frontend.log
   ```

## üß™ Protocolo de Testes de API

Para garantir o funcionamento adequado ap√≥s a troca de modelos, siga este protocolo de testes:

### 1. Verificar o servi√ßo Ollama
```bash
# Verificar se o servi√ßo Ollama est√° rodando
ps aux | grep ollama

# Se n√£o estiver rodando, iniciar o servi√ßo
ollama serve
```

### 2. Testar a conex√£o com o modelo
```bash
# Verificar os modelos dispon√≠veis
ollama list

# Testar o modelo diretamente via linha de comando
echo "Qual a capital do Brasil?" | ollama run NOME_DO_MODELO
```

### 3. Testar API do Backend
```bash
# Testar conex√£o b√°sica com o backend
curl http://localhost:8000/ | python -m json.tool

# Testar a API de perguntas
curl -X POST http://localhost:8000/perguntar \
  -H "Content-Type: application/json" \
  -d '{"pergunta":"Qual a capital do Brasil?"}' | python -m json.tool

# Testar a API de streaming (deve retornar um fluxo de dados)
curl -N -X POST http://localhost:8000/perguntar_stream \
  -H "Content-Type: application/json" \
  -d '{"pergunta":"Qual a capital do Brasil?"}'
```

### 4. Verificar Logs de Erros
```bash
# Verificar logs do backend por erros
grep -i "erro\|error\|exce√ß√£o\|exception" logs/backend.log | tail -n 20

# Verificar logs do frontend por erros
grep -i "erro\|error\|exce√ß√£o\|exception" logs/frontend.log | tail -n 20
```

### 5. Teste Completo no Navegador
1. Abra o navegador e acesse http://192.168.0.241:3000
2. Crie um novo chat e fa√ßa uma pergunta simples
3. Verifique se a resposta aparece corretamente, sem erros 500
4. Tente uma pergunta mais complexa para avaliar o desempenho do modelo

## ‚ö†Ô∏è Poss√≠veis Problemas e Solu√ß√µes

| Problema | Causa Prov√°vel | Solu√ß√£o |
|----------|----------------|---------|
| Erro 500 no servidor | Modelo especificado no .env n√£o existe | Verificar nome do modelo (`ollama list`) e corrigir no .env |
| Erro "Failed to fetch" | Backend n√£o est√° respondendo | Verificar logs do backend e reiniciar servi√ßos |
| Mensagem "operando em modo offline" | Problemas na comunica√ß√£o com Ollama | Verificar se o Ollama est√° rodando e responde a consultas |
| Muito tempo para responder | Par√¢metros inadequados para o modelo | Ajustar par√¢metros de contexto e threads no main_api.py |
| Respostas truncadas | Valor de num_predict muito baixo | Aumentar par√¢metro num_predict nas op√ß√µes |
| Backend usa modelo incorreto | Arquivo .env n√£o est√° sendo lido corretamente | Reiniciar completamente todos os servi√ßos, incluindo o backend |
| Falha no carregamento do modelo | Mem√≥ria insuficiente | Verificar uso de mem√≥ria e liberar recursos ou escolher modelo menor |

---

**¬© NEXORTECH - GPTPOL 2024** 